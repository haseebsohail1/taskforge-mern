# Code Review Approach
I usually start by asking what the change is trying to achieve and what risks it introduces. Then I scan for security, data integrity, and performance issues before I worry about style. I look for missing auth, weak access checks, unsafe serialization of fields like passwords, and places where ObjectId validation is skipped. I also make sure async errors flow into the global handler and that validation is consistent and centralized. After that, I review behavior: status codes, pagination defaults, and whether error messages are clear without leaking sensitive details. I check maintainability too, like naming, file boundaries, and whether the code is testable and reasonably decomposed. I expect tests for new behavior and I want to see at least one failure path covered, not just the happy path. If tests are missing, I request a minimal set that proves the change. My feedback is written in a simple what/why/how format so it is actionable and tied to user or operational impact. I leave ask comments when something is ambiguous and suggest comments when tradeoffs are optional. I also verify documentation updates and any migration notes. Finally, I do a quick pass for consistency with existing patterns and to spot opportunities to simplify.

# Mentoring Plan
My mentoring plan is about building confidence and independence. In week one, I pair with the engineer on a small feature and learn how they like to work. We agree on a short roadmap that includes both project goals and growth goals. Each week I schedule a focused session: one on system design, one on code quality, and one on product impact. I encourage them to explain their approach before coding so they practice reasoning and tradeoff thinking. I use reviews as a teaching tool, but I avoid nitpicking; I focus on the top two improvements and explain why they matter. I share a few reference patterns and ask them to implement or adapt them, which helps build muscle memory. I also set clear milestones like owning an endpoint end to end or designing a small schema change. Every two weeks we do a quick retro to discuss what is working, what is hard, and what support they need. For junior engineers I emphasize testing, error handling, and clear commit history. For senior growth I emphasize scope management, stakeholder communication, and raising risks early. The goal is that within three months they can deliver a feature from discovery through release with minimal supervision.

# Architecture Decisions
I structured the backend by config, middleware, models, controllers, and routes to keep responsibilities clear and easy to navigate. I chose JWT auth with a tokenVersion invalidation strategy because it is simple to reason about, works locally without extra infrastructure, and keeps logout predictable. The tradeoff is a user lookup per request, which is acceptable at this scale. Validation is handled with Joi so schemas are explicit and reused across routes, and the API fails fast with consistent error responses. I enforce team membership checks in controllers to prevent cross-team data leakage and keep authorization close to business logic. For tasks, only creators or admins can delete, which balances safety and clarity and is easy to document. Mongoose models are paired with TypeScript types to keep runtime and compile-time checks aligned. On the frontend, I used a lightweight AuthContext and Axios interceptors to keep auth flow simple and consistent. Protected routes are isolated from public routes, and the UI is built from small reusable components so pages stay thin. The folder structure mirrors backend concepts to reduce context switching and make onboarding faster. With more time, I would add OpenAPI docs and a small service layer. Overall, I favored maintainability and predictable behavior over clever abstractions.

# Scaling Strategy
At 10,000 concurrent users and 1 million tasks, I expect pressure on the database first: indexes, query shape, and payload size. I would start by profiling query patterns for task lists, team membership checks, and auth lookups. Then I would add compound indexes for the most common filters (teamId, status, priority, dueDate) and ensure team membership queries are indexed. I would limit heavy populates and use projections to reduce response size, plus add pagination defaults and hard caps. The tokenVersion check adds a user lookup per request, so I would add a short-lived cache for that field. I would scale API servers horizontally behind a load balancer and keep instances stateless. MongoDB would get a read replica for read-heavy endpoints and connection pooling tuned to the workload. I would add caching for common lookups like team lists and rate limit per user to protect capacity. Background work like notifications would go to a queue. On the frontend, I would introduce list virtualization for large task lists and add incremental loading. For observability, I would add structured logging, tracing, and dashboards for p95 latency, error rates, and slow queries. If growth continues, I would evaluate splitting auth and task services or adding an API gateway.

# Potential Improvements
Next, I would add refresh tokens with rotation, better audit logging, and more granular permissions at the team and task levels. I would also add task activity history, comments, and attachments with blob storage integration. From a UX angle, I would add optimistic updates, richer empty states, and saved filters. A stats dashboard with charts would help teams track throughput and bottlenecks. On the DevOps side, I would add a CI pipeline that runs linting and tests on every pull request, and a containerized local dev setup for faster onboarding. Finally, I would invest in accessibility reviews and internationalization so the product is ready for a broader user base.
